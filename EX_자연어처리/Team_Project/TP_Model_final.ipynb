{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 전처리된 파일들 통합\n",
    "folder_path = r\"C:\\Users\\kdp\\Desktop\\KDW\\EX_자연어처리\\D1010\\무한도전\"\n",
    "file_pattern = os.path.join(folder_path, '*.xlsx')\n",
    "\n",
    "all_dataframes = []\n",
    "for file in glob.glob(file_pattern):\n",
    "    df = pd.read_excel(file, header=None)\n",
    "    df.columns = ['episode', 'comment']  # 0번 컬럼을 'episode', 1번 컬럼을 'comment'로 설정\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# 텍스트 데이터와 라벨 추출\n",
    "texts = combined_df['comment'].tolist()\n",
    "labels = combined_df['episode'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1541\n"
     ]
    }
   ],
   "source": [
    "# 단어사전 만들기\n",
    "vocab = {}  # 어휘 사전\n",
    "for sentence in texts:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1  # 새로운 단어를 vocab에 추가하고 인덱스 부여\n",
    "\n",
    "vocab_size = len(vocab) + 1  # vocab 크기 설정\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정': 0, '형': 1, '돈': 2, ' ': 3, '도': 4, '진': 5, '짜': 6, '유': 7, '재': 8, '석': 9, '공': 10, '격': 11, '수': 12, '장': 13, '난': 14, '뒤': 15, '포': 16, '트': 17, '만': 18, '본': 19, '인': 20, '작': 21, '웃': 22, '기': 23, '행': 24, '혼': 25, '자': 26, '땀': 27, '범': 28, '벅': 29, '저': 30, '걸': 31, '느': 32, '님': 33, '당': 34, '신': 35, '대': 36, '체': 37, '부': 38, '분': 39, '준': 40, '하': 41, '말': 42, '투': 43, '존': 44, '똑': 45, '깜': 46, '짝': 47, '압': 48, '개': 49, '그': 50, '연': 51, '평': 52, '소': 53, '무': 54, '스': 55, '라': 56, '이': 57, '커': 58, '박': 59, '명': 60, '사': 61, '실': 62, '월': 63, '능': 64, '력': 65, '팀': 66, '위': 67, '해': 68, '에': 69, '피': 70, '드': 71, '안': 72, '냥': 73, '심': 74, '감': 75, '탄': 76, '간': 77, '방': 78, '송': 79, '최': 80, '고': 81, '다': 82, '레': 83, '전': 84, '리': 85, '액': 86, '션': 87, '억': 88, '지': 89, '옹': 90, '듯': 91, '넋': 92, '깔': 93, '거': 94, '게': 95, '요': 96, '얼': 97, '마': 98, '나': 99, '관': 100, '찰': 101, '렙': 102, '항': 103, '것': 104, '별': 105, '조': 106, '합': 107, '컨': 108, '텐': 109, '츠': 110, '획': 111, '보': 112, '금': 113, '버': 114, '릴': 115, '때': 116, '일': 117, '러': 118, '배': 119, '려': 120, '타': 121, '밍': 122, '순': 123, '발': 124, '임': 125, '노': 126, '홍': 127, '철': 128, '움': 129, '땐': 130, '생': 131, '각': 132, '서': 133, '브': 134, '멤': 135, '특': 136, '징': 137, '파': 138, '악': 139, '또': 140, '제': 141, '맨': 142, '예': 143, '국': 144, '민': 145, '왜': 146, '탑': 147, '는': 148, '동': 149, '중': 150, '갑': 151, '굴': 152, '처': 153, '큐': 154, '가': 155, '꿈': 156, '역': 157, '시': 158, '네': 159, '모': 160, '습': 161, '절': 162, '판': 163, '깝': 164, '닚': 165, '디': 166, '테': 167, '야': 168, '확': 169, '편': 170, '미': 171, '어': 172, '밬': 173, '람': 174, '음': 175, '봄': 176, '메': 177, '뿐': 178, '할': 179, '집': 180, '의': 181, '천': 182, '필': 183, '더': 184, '니': 185, '골': 186, '폭': 187, '로': 188, '과': 189, '황': 190, '줄': 191, '우': 192, '젠': 193, '프': 194, '캐': 195, '릭': 196, '터': 197, '점': 198, '화': 199, '면': 200, '밖': 201, '탭': 202, '툭': 203, '맴': 204, '들': 205, '얘': 206, '댓': 207, '글': 208, '즘': 209, '한': 210, '헌': 211, '콕': 212, '입': 213, '산': 214, '물': 215, '촬': 216, '영': 217, '참': 218, '살': 219, '손': 220, '묵': 221, '등': 222, '누': 223, '칭': 224, '찬': 225, '애': 226, '적': 227, '번': 228, '주': 229, '년': 230, '쉬': 231, '건': 232, '뚜': 233, '몇': 234, '표': 235, '율': 236, '혐': 237, '오': 238, '란': 239, '막': 240, '넼': 241, '김': 242, '태': 243, '호': 244, '런': 245, '상': 246, '너': 247, '완': 248, '댕': 249, '와': 250, '치': 251, '질': 252, '량': 253, '흉': 254, '내': 255, '성': 256, '차': 257, '떨': 258, '결': 259, '찌': 260, '롱': 261, '남': 262, '몸': 263, '름': 264, '죽': 265, '겠': 266, '껀': 267, '데': 268, '아': 269, '묘': 270, '짴': 271, '낼': 272, '든': 273, '괴': 274, '킬': 275, '링': 276, '싹': 277, '좀': 278, '출': 279, '틀': 280, '늘': 281, '급': 282, '단': 283, '풀': 284, '삐': 285, '놨': 286, '엌': 287, '변': 288, '꼽': 289, '계': 290, '속': 291, '램': 292, '눈': 293, '환': 294, '럼': 295, '열': 296, '씨': 297, '알': 298, '구': 299, '원': 300, '함': 301, '빛': 302, '듬': 303, '추': 304, '녹': 305, '식': 306, '은': 307, '춤': 308, '바': 309, '후': 310, '울': 311, '증': 312, '비': 313, '꼭': 314, '통': 315, '카': 316, '템': 317, '몫': 318, '용': 319, '새': 320, '졸': 321, '귀': 322, '근': 323, '뱅': 324, '크': 325, '꽤': 326, '뜻': 327, '경': 328, '선': 329, '택': 330, '현': 331, '즈': 332, '뭐': 333, '윗': 334, '른': 335, '초': 336, '반': 337, '갈': 338, '록': 339, '덜': 340, '째': 341, '홀': 342, '설': 343, '여': 344, '욕': 345, '두': 346, '흐': 347, '떄': 348, '래': 349, '젊': 350, '닝': 351, '활': 352, '토': 353, '꿀': 354, '잼': 355, '쫙': 356, '엠': 357, '끔': 358, '복': 359, '렉': 360, '벨': 361, '교': 362, '같': 363, '닼': 364, '향': 365, '맘': 366, '껏': 367, '세': 368, '젤': 369, '딜': 370, '탱': 371, '센': 372, '독': 373, '슬': 374, '픔': 375, '왤': 376, '케': 377, '쪼': 378, '코': 379, '잡': 380, '롤': 381, '워': 382, '접': 383, '먹': 384, '솔': 385, '머': 386, '볼': 387, '삭': 388, '녁': 389, '뎈': 390, '앞': 391, '옛': 392, '날': 393, '외': 394, '불': 395, '약': 396, '쩐': 397, '플': 398, '낌': 399, '랄': 400, '채': 401, '티': 402, '광': 403, '첵': 404, '십': 405, '쇼': 406, '팁': 407, '곀': 408, '짱': 409, '깨': 410, '길': 411, '찔': 412, '떡': 413, '권': 414, '꽁': 415, '루': 416, '섭': 417, '렵': 418, '콘': 419, '색': 420, '웤': 421, '클': 422, '린': 423, '회': 424, '잭': 425, '슨': 426, '블': 427, '랙': 428, '뮤': 429, '끝': 430, '효': 431, '양': 432, '따': 433, '팬': 434, '삼': 435, '빈': 436, '청': 437, '멘': 438, '운': 439, '쩜': 440, '썰': 441, '문': 442, '베': 443, '승': 444, '료': 445, '벽': 446, '톤': 447, '언': 448, '냉': 449, '탁': 450, '겁': 451, '강': 452, '쪽': 453, '끈': 454, '밤': 455, '놈': 456, '댄': 457, '밋': 458, '극': 459, '쓴': 460, '헛': 461, '논': 462, '업': 463, '혈': 464, '뇌': 465, '잍': 466, '꼬': 467, '끄': 468, '르': 469, '떼': 470, '늠': 471, '덕': 472, '룡': 473, '앟': 474, '빡': 475, '및': 476, '욬': 477, '락': 478, '히': 479, '농': 480, '얔': 481, '쳣': 482, '빵': 483, '립': 484, '굿': 485, '맹': 486, '침': 487, '뺨': 488, '닠': 489, '짘': 490, '땜': 491, '잔': 492, '앸': 493, '녕': 494, '싸': 495, '빙': 496, '젝': 497, '퓨': 498, '족': 499, '맥': 500, '잠': 501, '힘': 502, '뭔': 503, '엽': 504, '콩': 505, '희': 506, '냌': 507, '싱': 508, '킄': 509, '옥': 510, '매': 511, '밥': 512, '쭈': 513, '욱': 514, '햐': 515, '뉘': 516, '앙': 517, '띠': 518, '군': 519, '쿨': 520, '킹': 521, '뜬': 522, '짬': 523, '욜': 524, '딱': 525, '촌': 526, '쫄': 527, '졋': 528, '옆': 529, '꾸': 530, '듣': 531, '롴': 532, '랑': 533, '멍': 534, '핑': 535, '직': 536, '딩': 537, '퀵': 538, '빅': 539, '뫼': 540, '쥐': 541, '곧': 542, '잘': 543, '몰': 544, '랔': 545, '캬': 546, '칼': 547, '땨': 548, '풍': 549, '키': 550, '엊': 551, '끼': 552, '썸': 553, '넬': 554, '목': 555, '털': 556, '써': 557, '닥': 558, '균': 559, '벌': 560, '학': 561, '펙': 562, '멋': 563, '백': 564, '뎌': 565, '못': 566, '휠': 567, '씬': 568, '땅': 569, '몆': 570, '친': 571, '줌': 572, '넣': 573, '깽': 574, '척': 575, '섯': 576, '멱': 577, '규': 578, '폿': 579, '픽': 580, '있': 581, '놀': 582, '빠': 583, '쌍': 584, '옜': 585, '쿵': 586, '충': 587, '컷': 588, '뉵': 589, '킵': 590, '냨': 591, '창': 592, '검': 593, '깐': 594, '폼': 595, '델': 596, '갯': 597, '윤': 598, '종': 599, '담': 600, '겄': 601, '맛': 602, '잇': 603, '캌': 604, '켴': 605, '귓': 606, '좋': 607, '았': 608, '넴': 609, '툴': 610, '험': 611, '탕': 612, '률': 613, '림': 614, '달': 615, '앀': 616, '총': 617, '렁': 618, '뚱': 619, '흠': 620, '돌': 621, '셉': 622, '숙': 623, '훈': 624, '엉': 625, '덩': 626, '곤': 627, '긴': 628, '뎅': 629, '잌': 630, '술': 631, '먼': 632, '퀄': 633, '었': 634, '깈': 635, '듴': 636, '빼': 637, '던': 638, '팔': 639, '뚝': 640, '쳤': 641, '답': 642, '톱': 643, '휘': 644, '튜': 645, '값': 646, '둘': 647, '뉴': 648, '랜': 649, '꽃': 650, '갸': 651, '뻔': 652, '씌': 653, '윳': 654, '겨': 655, '뒹': 656, '됏': 657, '쁘': 658, '칫': 659, '텝': 660, '퀴': 661, '잖': 662, '앜': 663, '랬': 664, '익': 665, '쩌': 666, '론': 667, '탈': 668, '육': 669, '법': 670, '왼': 671, '뒷': 672, '북': 673, '온': 674, '넥': 675, '낙': 676, '탓': 677, '옴': 678, '겻': 679, '눜': 680, '셧': 681, '핏': 682, '짐': 683, '죨': 684, '렷': 685, '젋': 686, '뭍': 687, '혀': 688, '슈': 689, '퍼': 690, '옷': 691, '망': 692, '령': 693, '뺴': 694, '을': 695, '챢': 696, '빨': 697, '껄': 698, '뭘': 699, '취': 700, '갘': 701, '텅': 702, '틐': 703, '올': 704, '숨': 705, '묻': 706, '혘': 707, '낄': 708, '렸': 709, '봨': 710, '된': 711, '흔': 712, '넹': 713, '촤': 714, '헬': 715, '멧': 716, '흑': 717, '씀': 718, '까': 719, '젼': 720, '례': 721, '쯤': 722, '횟': 723, '듀': 724, '룸': 725, '널': 726, '엄': 727, '웡': 728, '휴': 729, '병': 730, '련': 731, '쟤': 732, '룤': 733, '럭': 734, '줠': 735, '줫': 736, '페': 737, '앱': 738, '쓰': 739, '졐': 740, '햌': 741, '혜': 742, '꾼': 743, '럿': 744, '좌': 745, '겹': 746, '윾': 747, '곸': 748, '썪': 749, '핫': 750, '많': 751, '높': 752, '돼': 753, '팅': 754, '꽝': 755, '밀': 756, '칠': 757, '썬': 758, '봉': 759, '똥': 760, '퀘': 761, '쟁': 762, '삶': 763, '꾹': 764, '셔': 765, '퐄': 766, '잣': 767, '윽': 768, '꼴': 769, '쏜': 770, '허': 771, '릎': 772, '쌤': 773, '궄': 774, '쫌': 775, '쳐': 776, '빌': 777, '뿍': 778, '틈': 779, '얄': 780, '헐': 781, '숫': 782, '껑': 783, '찝': 784, '힌': 785, '웠': 786, '며': 787, '뇨': 788, '쥼': 789, '책': 790, '견': 791, '죄': 792, '랰': 793, '핰': 794, '궁': 795, '했': 796, '짤': 797, '밌': 798, '띵': 799, '즌': 800, '쫀': 801, '훌': 802, '염': 803, '첨': 804, '섴': 805, '껰': 806, '엨': 807, '댘': 808, '싼': 809, '햏': 810, '잉': 811, '없': 812, '슴': 813, '틸': 814, '찍': 815, '곡': 816, '괔': 817, '귴': 818, '옄': 819, '넌': 820, '엇': 821, '닮': 822, '뿜': 823, '갔': 824, '갓': 825, '늬': 826, '꽉': 827, '줮': 828, '콧': 829, '읍': 830, '앋': 831, '퉁': 832, '쾅': 833, '팍': 834, '촠': 835, '멜': 836, '쟈': 837, '짚': 838, '읏': 839, '패': 840, '쉼': 841, '쿠': 842, '꼼': 843, '봤': 844, '끅': 845, '응': 846, '으': 847, '뱃': 848, '품': 849, '힛': 850, '략': 851, '깤': 852, '즁': 853, '횽': 854, '뜰': 855, '덟': 856, '왔': 857, '핻': 858, '둥': 859, '맑': 860, '쑥': 861, '칸': 862, '숱': 863, '쥰': 864, '협': 865, '윙': 866, '큮': 867, '닻': 868, '뷰': 869, '엔': 870, '폴': 871, '셨': 872, '됐': 873, '읗': 874, '앶': 875, '봇': 876, '득': 877, '뎋': 878, '펀': 879, '랭': 880, '맄': 881, '곜': 882, '팩': 883, '꿍': 884, '왕': 885, '곱': 886, '킨': 887, '눔': 888, '눗': 889, '첳': 890, '혹': 891, '걱': 892, '팽': 893, '맊': 894, '낭': 895, '뭌': 896, '솜': 897, '컴': 898, '땔': 899, '핚': 900, '윶': 901, '뢰': 902, '객': 903, '숭': 904, '튼': 905, '닌': 906, '궐': 907, '듸': 908, '얌': 909, '젇': 910, '헤': 911, '핸': 912, '폰': 913, '휙': 914, '긍': 915, '겸': 916, '밸': 917, '찮': 918, '턱': 919, '였': 920, '넛': 921, '콸': 922, '욥': 923, '럽': 924, '폐': 925, '셋': 926, '닉': 927, '겈': 928, '축': 929, '측': 930, '냄': 931, '칙': 932, '맷': 933, '왓': 934, '냐': 935, '툽': 936, '륜': 937, '녜': 938, '갠': 939, '뼈': 940, '랩': 941, '틱': 942, '녀': 943, '웅': 944, '핀': 945, '캠': 946, '옵': 947, '짓': 948, '훗': 949, '훼': 950, '흘': 951, '끗': 952, '퇴': 953, '돜': 954, '뻑': 955, '욤': 956, '쌩': 957, '햇': 958, '쨋': 959, '엘': 960, '겐': 961, '븐': 962, '푹': 963, '넷': 964, '긔': 965, '곰': 966, '팡': 967, '쨌': 968, '떳': 969, '멀': 970, '뽀': 971, '촥': 972, '착': 973, '첬': 974, '숰': 975, '섬': 976, '맣': 977, '뭣': 978, '캡': 979, '킥': 980, '윈': 981, '깥': 982, '끌': 983, '벤': 984, '져': 985, '솨': 986, '캉': 987, '봣': 988, '쭉': 989, '념': 990, '깃': 991, '랍': 992, '뿌': 993, '갖': 994, '죵': 995, '닷': 996, '룹': 997, '뎃': 998, '첫': 999, '밴': 1000, '엑': 1001, '샛': 1002, '웹': 1003, '룻': 1004, '롯': 1005, '릉': 1006, '꼇': 1007, '껴': 1008, '닽': 1009, '뻘': 1010, '콤': 1011, '셜': 1012, '턴': 1013, '딸': 1014, '굳': 1015, '킴': 1016, '웨': 1017, '뀌': 1018, '낮': 1019, '핵': 1020, '탐': 1021, '를': 1022, '삿': 1023, '빽': 1024, '힐': 1025, '꽂': 1026, '쟼': 1027, '쥬': 1028, '캔': 1029, '류': 1030, '붐': 1031, '팝': 1032, '뭇': 1033, '잎': 1034, '텔': 1035, '뷔': 1036, '암': 1037, '쌔': 1038, '홈': 1039, '춥': 1040, '찢': 1041, '낫': 1042, '땡': 1043, '쩍': 1044, '덤': 1045, '톡': 1046, '낯': 1047, '쇠': 1048, '넨': 1049, '륙': 1050, '컬': 1051, '납': 1052, '겉': 1053, '쒸': 1054, '랏': 1055, '쑈': 1056, '썻': 1057, '튀': 1058, '둑': 1059, '툯': 1060, '펑': 1061, '곳': 1062, '앗': 1063, '벙': 1064, '낰': 1065, '혁': 1066, '께': 1067, '팰': 1068, '틴': 1069, '헳': 1070, '뻥': 1071, '흙': 1072, '읔': 1073, '톨': 1074, '꺄': 1075, '룰': 1076, '굽': 1077, '맏': 1078, '뻣': 1079, '읶': 1080, '둔': 1081, '챙': 1082, '쉽': 1083, '콜': 1084, '쎄': 1085, '딬': 1086, '뜨': 1087, '뿡': 1088, '웰': 1089, '찾': 1090, '죸': 1091, '즐': 1092, '넘': 1093, '헝': 1094, '큼': 1095, '밉': 1096, '숯': 1097, '욨': 1098, '걌': 1099, '잨': 1100, '퍽': 1101, '샴': 1102, '푸': 1103, '킷': 1104, '텦': 1105, '젭': 1106, '렛': 1107, '깅': 1108, '옶': 1109, '땤': 1110, '멩': 1111, '귕': 1112, '얍': 1113, '혓': 1114, '훤': 1115, '붙': 1116, '콥': 1117, '뱀': 1118, '횃': 1119, '똘': 1120, '릿': 1121, '층': 1122, '멸': 1123, '뀔': 1124, '웟': 1125, '뿔': 1126, '랐': 1127, '좆': 1128, '좃': 1129, '되': 1130, '샤': 1131, '삥': 1132, '찐': 1133, '샙': 1134, '옮': 1135, '봐': 1136, '꺽': 1137, '겟': 1138, '렴': 1139, '놓': 1140, '뽑': 1141, '쏙': 1142, '뤼': 1143, '돗': 1144, '젖': 1145, '얻': 1146, '샐': 1147, '숟': 1148, '흥': 1149, '흡': 1150, '훅': 1151, '뫠': 1152, '쌀': 1153, '엮': 1154, '펔': 1155, '룈': 1156, '큨': 1157, '앤': 1158, '찡': 1159, '긋': 1160, '푼': 1161, '읃': 1162, '맞': 1163, '줜': 1164, '쇄': 1165, '쌉': 1166, '엣': 1167, '둴': 1168, '돠': 1169, '샘': 1170, '롭': 1171, '섀': 1172, '뭭': 1173, '눅': 1174, '펄': 1175, '쩔': 1176, '큰': 1177, '썹': 1178, '벗': 1179, '왘': 1180, '퉄': 1181, '힙': 1182, '홬': 1183, '몬': 1184, '셈': 1185, '뜸': 1186, '믹': 1187, '쏘': 1188, '윸': 1189, '졌': 1190, '켜': 1191, '렬': 1192, '싶': 1193, '풋': 1194, '쌈': 1195, '꺼': 1196, '삑': 1197, '뭬': 1198, '뫩': 1199, '롬': 1200, '므': 1201, '멬': 1202, '홭': 1203, '펌': 1204, '뫽': 1205, '쉐': 1206, '닛': 1207, '뮄': 1208, '춋': 1209, '밭': 1210, '렌': 1211, '걐': 1212, '컵': 1213, '뎁': 1214, '빜': 1215, '톰': 1216, '믐': 1217, '읕': 1218, '즉': 1219, '롼': 1220, '났': 1221, '뿟': 1222, '쁜': 1223, '쳘': 1224, '읺': 1225, '앉': 1226, '훨': 1227, '렼': 1228, '샵': 1229, '닭': 1230, '젓': 1231, '쳠': 1232, '껍': 1233, '뫄': 1234, '웩': 1235, '릇': 1236, '늪': 1237, '줭': 1238, '낚': 1239, '햄': 1240, '썩': 1241, '랫': 1242, '셐': 1243, '벚': 1244, '햍': 1245, '숩': 1246, '꼰': 1247, '쏵': 1248, '빰': 1249, '뮌': 1250, '넽': 1251, '탠': 1252, '샄': 1253, '밑': 1254, '뾰': 1255, '딘': 1256, '짲': 1257, '윰': 1258, '쥘': 1259, '썽': 1260, '딧': 1261, '믈': 1262, '땸': 1263, '텍': 1264, '룩': 1265, '뇽': 1266, '샷': 1267, '곁': 1268, '횐': 1269, '앳': 1270, '찜': 1271, '챠': 1272, '꼌': 1273, '녘': 1274, '캅': 1275, '샌': 1276, '몽': 1277, '깄': 1278, '쮸': 1279, '섹': 1280, '깡': 1281, '츤': 1282, '샹': 1283, '쉣': 1284, '댐': 1285, '쩖': 1286, '땃': 1287, '킼': 1288, '힝': 1289, '쌥': 1290, '붸': 1291, '씹': 1292, '섵': 1293, '뺏': 1294, '컄': 1295, '윜': 1296, '뜌': 1297, '싀': 1298, '뭉': 1299, '똬': 1300, '엍': 1301, '놬': 1302, '욧': 1303, '뗬': 1304, '퀜': 1305, '툰': 1306, '탬': 1307, '춘': 1308, '셀': 1309, '됔': 1310, '뿝': 1311, '빗': 1312, '컥': 1313, '귘': 1314, '쳨': 1315, '눞': 1316, '꿰': 1317, '퀭': 1318, '웣': 1319, '웍': 1320, '낏': 1321, '욀': 1322, '앵': 1323, '릅': 1324, '켄': 1325, '꽐': 1326, '땈': 1327, '빱': 1328, '뽜': 1329, '롸': 1330, '땟': 1331, '깟': 1332, '슥': 1333, '늑': 1334, '쾡': 1335, '쉨': 1336, '셰': 1337, '먄': 1338, '넺': 1339, '뫀': 1340, '슼': 1341, '믿': 1342, '컹': 1343, '싫': 1344, '쾌': 1345, '킁': 1346, '쳌': 1347, '빔': 1348, '떴': 1349, '쥴': 1350, '팥': 1351, '핡': 1352, '훔': 1353, '믓': 1354, '볕': 1355, '흣': 1356, '곢': 1357, '꿋': 1358, '쬬': 1359, '늼': 1360, '갱': 1361, '듭': 1362, '뽕': 1363, '숵': 1364, '숴': 1365, '몀': 1366, '벼': 1367, '셬': 1368, '훙': 1369, '잊': 1370, '헨': 1371, '픂': 1372, '받': 1373, '겜': 1374, '엎': 1375, '줗': 1376, '븜': 1377, '뚤': 1378, '멥': 1379, '튭': 1380, '닐': 1381, '콬': 1382, '씼': 1383, '덧': 1384, '찹': 1385, '퀸': 1386, '앨': 1387, '펠': 1388, '왈': 1389, '칵': 1390, '뼛': 1391, '딴': 1392, '늦': 1393, '켓': 1394, '옌': 1395, '깊': 1396, '쵝': 1397, '쨔': 1398, '픈': 1399, '툼': 1400, '엿': 1401, '챔': 1402, '눌': 1403, '륨': 1404, '걔': 1405, '먗': 1406, '펏': 1407, '칮': 1408, '괜': 1409, '뼉': 1410, '쿤': 1411, '싄': 1412, '빚': 1413, '팸': 1414, '됫': 1415, '줏': 1416, '쨤': 1417, '콱': 1418, '횡': 1419, '돋': 1420, '놋': 1421, '삘': 1422, '탶': 1423, '솝': 1424, '촐': 1425, '융': 1426, '밈': 1427, '멕': 1428, '쟌': 1429, '솰': 1430, '붕': 1431, '볶': 1432, '헀': 1433, '쏱': 1434, '쿸': 1435, '쿡': 1436, '떈': 1437, '퐁': 1438, '쌋': 1439, '쿄': 1440, '쿜': 1441, '쿗': 1442, '웈': 1443, '캣': 1444, '꿎': 1445, '겤': 1446, '귤': 1447, '켘': 1448, '겼': 1449, '즙': 1450, '죜': 1451, '퍈': 1452, '궈': 1453, '씦': 1454, '끆': 1455, '둣': 1456, '믕': 1457, '냬': 1458, '띄': 1459, '붘': 1460, '쌌': 1461, '쉭': 1462, '썅': 1463, '눴': 1464, '뱍': 1465, '뭡': 1466, '겡': 1467, '갰': 1468, '봅': 1469, '뭄': 1470, '쓱': 1471, '않': 1472, '똫': 1473, '궤': 1474, '끊': 1475, '뺑': 1476, '곽': 1477, '칰': 1478, '꿩': 1479, '뗑': 1480, '먕': 1481, '긐': 1482, '솤': 1483, '띨': 1484, '켱': 1485, '돕': 1486, '팃': 1487, '짢': 1488, '쓸': 1489, '샠': 1490, '쌐': 1491, '맠': 1492, '눙': 1493, '뱉': 1494, '꾀': 1495, '쟨': 1496, '짖': 1497, '럴': 1498, '뤀': 1499, '텁': 1500, '폔': 1501, '쨕': 1502, '촛': 1503, '뤠': 1504, '즤': 1505, '짆': 1506, '뽷': 1507, '곂': 1508, '딨': 1509, '킇': 1510, '늫': 1511, '핱': 1512, '긿': 1513, '뜽': 1514, '깍': 1515, '쨍': 1516, '홉': 1517, '듕': 1518, '챳': 1519, '펜': 1520, '둡': 1521, '텀': 1522, '랴': 1523, '퀀': 1524, '슠': 1525, '댴': 1526, '숄': 1527, '쓔': 1528, '뜾': 1529, '빸': 1530, '멈': 1531, '큿': 1532, '칩': 1533, '짊': 1534, '귄': 1535, '렐': 1536, '쏠': 1537, '빳': 1538, '옾': 1539}\n"
     ]
    }
   ],
   "source": [
    "# json 파일로 저장\n",
    "import json\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "\n",
    "with open(\"vocab.json\", \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "vocab_dict = {}\n",
    "\n",
    "for idx, word in enumerate(vocab):\n",
    "    vocab_dict[word] = idx\n",
    "print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.8325, Train Accuracy: 0.2636, Validation Loss: 3.0305, Validation Accuracy: 0.2755\n",
      "Validation loss : 3.0305\n",
      "Epoch [2/100], Train Loss: 2.9618, Train Accuracy: 0.2619, Validation Loss: 3.2131, Validation Accuracy: 0.2755\n",
      "Epoch [3/100], Train Loss: 2.8486, Train Accuracy: 0.2583, Validation Loss: 3.4028, Validation Accuracy: 0.1145\n",
      "Epoch [4/100], Train Loss: 2.7140, Train Accuracy: 0.2584, Validation Loss: 3.4682, Validation Accuracy: 0.2755\n",
      "3에포크 만큼 성능 향상이 되지 않아 학습을 종료합니다.\n",
      "Test Accuracy: 0.2887\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_len = 20\n",
    "encoded_texts = []\n",
    "for sentence in texts:\n",
    "    encoded = [vocab[word] for word in sentence]\n",
    "    if len(encoded) < max_len:\n",
    "        encoded += [0] * (max_len - len(encoded))  # 패딩\n",
    "    else:\n",
    "        encoded = encoded[:max_len]\n",
    "    encoded_texts.append(encoded)\n",
    "\n",
    "# 라벨을 문자열로 변환하여 타입 일관성 유지\n",
    "labels = [str(label) for label in labels]\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_to_index = {label: idx for idx, label in enumerate(sorted(set(labels)))}\n",
    "indexed_labels = [label_to_index[label] for label in labels]\n",
    "\n",
    "# PyTorch Dataset 정의\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_texts, indexed_labels, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CommentDataset(X_train, y_train)\n",
    "valid_dataset = CommentDataset(X_valid, y_valid)\n",
    "test_dataset = CommentDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # 양방향 LSTM의 두 방향 결합\n",
    "  \n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# 모델, 손실 함수 및 옵티마이저 초기화\n",
    "embedding_dim = 300  \n",
    "hidden_dim = 512  # 히든 유닛 수 증가\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, verbose=True)\n",
    "\n",
    "# Early Stopping 설정\n",
    "best_loss = float('inf')\n",
    "patience, trigger = 3, 0\n",
    "\n",
    "# 모델 학습\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파 및 옵티마이저 스텝\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 정확도 계산\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # 검증 손실 계산\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val, total_val = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in valid_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # 정확도 계산\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    # 에포크별 결과 출력\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        trigger = 0\n",
    "        print(f'Validation loss : {best_loss:.4f}')\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        trigger += 1\n",
    "        if trigger >= patience:\n",
    "            print(f'{patience}에포크 만큼 성능 향상이 되지 않아 학습을 종료합니다.')\n",
    "            break\n",
    "\n",
    "\n",
    "# 최종 모델 평가\n",
    "model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTMClassifier:\n\tUnexpected key(s) in state_dict: \"embedding.weight\", \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\", \"lstm.weight_ih_l2_reverse\", \"lstm.weight_hh_l2_reverse\", \"lstm.bias_ih_l2_reverse\", \"lstm.bias_hh_l2_reverse\", \"lstm.weight_ih_l3\", \"lstm.weight_hh_l3\", \"lstm.bias_ih_l3\", \"lstm.bias_hh_l3\", \"lstm.weight_ih_l3_reverse\", \"lstm.weight_hh_l3_reverse\", \"lstm.bias_ih_l3_reverse\", \"lstm.bias_hh_l3_reverse\", \"lstm.weight_ih_l4\", \"lstm.weight_hh_l4\", \"lstm.bias_ih_l4\", \"lstm.bias_hh_l4\", \"lstm.weight_ih_l4_reverse\", \"lstm.weight_hh_l4_reverse\", \"lstm.bias_ih_l4_reverse\", \"lstm.bias_hh_l4_reverse\". \n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 400]) from checkpoint, the shape in current model is torch.Size([1024, 1000]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 400]) from checkpoint, the shape in current model is torch.Size([1024, 1000]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1201, 512]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1201]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(input_dim, hidden_dim, output_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 학습된 가중치 로드 (가중치 파일 이름을 적절하게 지정하세요)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_lstm_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# 예측 모드로 전환\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 예측 함수 정의\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL-PC\\.conda\\envs\\EXAM_DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTMClassifier:\n\tUnexpected key(s) in state_dict: \"embedding.weight\", \"lstm.weight_ih_l2\", \"lstm.weight_hh_l2\", \"lstm.bias_ih_l2\", \"lstm.bias_hh_l2\", \"lstm.weight_ih_l2_reverse\", \"lstm.weight_hh_l2_reverse\", \"lstm.bias_ih_l2_reverse\", \"lstm.bias_hh_l2_reverse\", \"lstm.weight_ih_l3\", \"lstm.weight_hh_l3\", \"lstm.bias_ih_l3\", \"lstm.bias_hh_l3\", \"lstm.weight_ih_l3_reverse\", \"lstm.weight_hh_l3_reverse\", \"lstm.bias_ih_l3_reverse\", \"lstm.bias_hh_l3_reverse\", \"lstm.weight_ih_l4\", \"lstm.weight_hh_l4\", \"lstm.bias_ih_l4\", \"lstm.bias_hh_l4\", \"lstm.weight_ih_l4_reverse\", \"lstm.weight_hh_l4_reverse\", \"lstm.bias_ih_l4_reverse\", \"lstm.bias_hh_l4_reverse\". \n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([1024, 400]) from checkpoint, the shape in current model is torch.Size([1024, 1000]).\n\tsize mismatch for lstm.weight_ih_l0_reverse: copying a param with shape torch.Size([1024, 400]) from checkpoint, the shape in current model is torch.Size([1024, 1000]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1201, 512]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([1201]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# 데이터 준비 (벡터라이저와 벡터 데이터 로드)\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "# 회차별 라벨 정의\n",
    "episode_names = [\n",
    "    \"무인도 특집\", \"무한상사 면접편\", \"무한상사 야유회\", \n",
    "    \"방콕 특집\", \"서해안고속도로가요제\", \"인생극장 특집\",\n",
    "    \"죄와 길\", \"짝꿍 특집\", \"네멋대로해라\", \"명수는 12살\"\n",
    "]\n",
    "\n",
    "# 라벨 인덱스와 회차 이름 매핑\n",
    "label_to_index = {name: idx for idx, name in enumerate(episode_names)}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "# LSTM 모델 정의 (가중치 로드를 위해 정의만 포함)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM expects a 3D tensor (batch_size, seq_len, input_dim)\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, input_dim) 형태로 변환\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)  # 양방향 LSTM 결합\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# 모델 생성\n",
    "input_dim = 1000  # TF-IDF 벡터의 feature dimension (적절한 값으로 설정)\n",
    "hidden_dim = 256\n",
    "output_dim = len(label_to_index)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# 학습된 가중치 로드 (가중치 파일 이름을 적절하게 지정하세요)\n",
    "model.load_state_dict(torch.load('best_lstm_model.pth', map_location=device))\n",
    "model.eval()  # 예측 모드로 전환\n",
    "\n",
    "# 예측 함수 정의\n",
    "def predict_episode(comment):\n",
    "    # TF-IDF 벡터화 및 텐서 변환\n",
    "    comment_vector = vectorizer.transform([comment])\n",
    "    comment_tensor = torch.tensor(comment_vector.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        output = model(comment_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return index_to_label[predicted_class]\n",
    "\n",
    "# 사용자로부터 입력받아 예측 수행\n",
    "new_comment = input(\"댓글을 입력하세요: \")\n",
    "predicted_episode = predict_episode(new_comment)\n",
    "print(f\"이 댓글은 '{predicted_episode}'과(와) 관련이 있습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_TEXT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
