{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 정의 모델 클래스\n",
    "- 부모클래스 : nn.Module\n",
    "- 필수오버라이딩 \n",
    "    *  _ _init_ _() : 모델 층 구성 즉, 설계\n",
    "    * forward() : 순방향 학습 진행 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩\n",
    "import torch                                # 텐서 관련 모듈\n",
    "import torch.nn as nn                       # 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F             # 인공신경망 관련 함수들 모듈 (손실함수, 활성화함수 등등)\n",
    "import torch.optim as optim                 # 최적화 관련 모듈(가중치, 절편 빠르게 찾아주는 알고리즘)\n",
    "from torchinfo import summary               # 모델 구조 및 정보 관련 모듈\n",
    "from torchmetrics.regression import *       # 회귀 성능 지표 관련 모듈\n",
    "from torchmetrics.classification import *   # 분류 성능 지표 관련 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [기본] 신경망 클래스  <hr>\n",
    "    * 입력층 - 입력 :  피쳐 수 고정\n",
    "    * 출력층 - 출력 :  타겟 수 고정\n",
    "    * 은닉층 - 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "# 데이터셋 : 피쳐 4개, 타겟 1개, 회귀\n",
    "# 입력층 : 입력 4개    출력 20개    AF ReLU \n",
    "# 은닉층 : 입력 20개   출력 100개   AF ReLU\n",
    "# 출력층 : 입력 100개  출력  1개    AF X, Sigmoid & Softmax     ### Sigmoid, Softmax는 분류일때 쓴다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(4,20)        # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20,100)     # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.out_layer=nn.Linear(100,1)         # W 100 + b 1 => 1P, 101*1 = 101개 변수수\n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수) \n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y=self.input_layer(x)   # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)             # y>=0 ---> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y)  # 1개 퍼셉트론 : y=x1W1+x2W2+...+x20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.out_layer(y)     #1개 퍼셉트론 : y=x1W1+x2W2+...+x100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수가 동적인 모델\n",
    "class MyModel2(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self,in_feature):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature,20)        # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(20,100)     # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.out_layer=nn.Linear(100,1)         # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수) \n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y=self.input_layer(x)   # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)             # y>=0 ---> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y)  # 1개 퍼셉트론 : y=x1W1+x2W2+...+x20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.out_layer(y)     #1개 퍼셉트론 : y=x1W1+x2W2+...+x100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 피쳐 수, 은닉층 퍼셉트론 수가 동적인 모델\n",
    "class MyModel3(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self,in_feature, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature,in_out)        # W 4 + b 1 => 1P, 5*20 = 100개 변수\n",
    "        self.hidden_layer=nn.Linear(in_out,h_out)     # W 20 + b 1 => 1P, 21*100 = 2100개 변수\n",
    "        self.out_layer=nn.Linear(h_out,1)         # W 100 + b 1 => 1P, 101*1 = 101개 변수\n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수) \n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y=self.input_layer(x)   # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)             # y>=0 ---> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y)  # 1개 퍼셉트론 : y=x1W1+x2W2+...+x20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.out_layer(y)     #1개 퍼셉트론 : y=x1W1+x2W2+...+x100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층의 개수가 동적인 모델\n",
    "class MyModel4(nn.Module):\n",
    "    \n",
    "    # 인스턴스/객체 생성 시 자동호출되는 메서드 (콜백함수 Callback func)\n",
    "    def __init__(self,in_feature, in_out, h_out):\n",
    "        # 부모클래스 생성\n",
    "        super().__init__()\n",
    "        # 자식클래스의 인스턴스 속성 설정\n",
    "        self.input_layer=nn.Linear(in_feature,in_out)\n",
    "        for i in range(?):\n",
    "           self.hidden_layer({i})=nn.Linear({i}_out,{i+1}_out)    \n",
    "        self.out_layer=nn.Linear(h_out,1)         \n",
    "        \n",
    "    # 순방향/전방향 학습 진행 시 자동호출되는 메서드 (콜백함수 Callback func : 시스템에서 호출되는 함수) \n",
    "    # 전달 인자 : 학습용 데이터셋\n",
    "    def forward(self, x):\n",
    "        \n",
    "        y=self.input_layer(x)   # 1개 퍼셉트론 : y=x1W1+x2W2+x3W3+x4W4+b\n",
    "        y=F.relu(y)             # y>=0 ---> 죽은 relu ==> leakyReLU\n",
    "\n",
    "        y=self.hidden_layer(y)  # 1개 퍼셉트론 : y=x1W1+x2W2+...+x20W20+b\n",
    "        y=F.relu(y)\n",
    "\n",
    "        return self.out_layer(y)     #1개 퍼셉트론 : y=x1W1+x2W2+...+x100W100+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 고정\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 텐서 저장 및 실행 위치 설정\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel3(\n",
       "  (input_layer): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (hidden_layer): Linear(in_features=50, out_features=30, bias=True)\n",
       "  (out_layer): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 모델 인스턴스 생성\n",
    "m1=MyModel3(3,50,30)\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input_layer.weight', Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119],\n",
      "        [ 0.2710, -0.5435,  0.3462],\n",
      "        [-0.1188,  0.2937,  0.0803],\n",
      "        [-0.0707,  0.1601,  0.0285],\n",
      "        [ 0.2109, -0.2250, -0.0421],\n",
      "        [-0.0520,  0.0837, -0.0023],\n",
      "        [ 0.5047,  0.1797, -0.2150],\n",
      "        [-0.3487, -0.0968, -0.2490],\n",
      "        [-0.1850,  0.0276,  0.3442],\n",
      "        [ 0.3138, -0.5644,  0.3579],\n",
      "        [ 0.1613,  0.5476,  0.3811],\n",
      "        [-0.5260, -0.5489, -0.2785],\n",
      "        [ 0.5070, -0.0962,  0.2471],\n",
      "        [-0.2683,  0.5665, -0.2443],\n",
      "        [ 0.4330,  0.0068, -0.3042],\n",
      "        [ 0.2968, -0.3065,  0.1698],\n",
      "        [-0.1667, -0.0633, -0.5551],\n",
      "        [-0.2753,  0.3133, -0.1403],\n",
      "        [ 0.5751,  0.4628, -0.0270],\n",
      "        [-0.3854,  0.3516,  0.1792],\n",
      "        [-0.3732,  0.3750,  0.3505],\n",
      "        [ 0.5120, -0.3236, -0.0950],\n",
      "        [-0.0112,  0.0843, -0.4382],\n",
      "        [-0.4097,  0.3141, -0.1354],\n",
      "        [ 0.2820,  0.0329,  0.1896],\n",
      "        [ 0.1270,  0.2099,  0.2862],\n",
      "        [-0.5347,  0.2906, -0.4059],\n",
      "        [-0.4356,  0.0351, -0.0984],\n",
      "        [ 0.3391, -0.3344, -0.5133],\n",
      "        [ 0.4202, -0.0856,  0.3247],\n",
      "        [ 0.1856, -0.4329,  0.1160],\n",
      "        [ 0.1387, -0.3866, -0.2739],\n",
      "        [ 0.1969,  0.1034, -0.2456],\n",
      "        [-0.1748,  0.5288, -0.1068],\n",
      "        [ 0.3255,  0.2500, -0.3732],\n",
      "        [-0.4910,  0.5542,  0.0301],\n",
      "        [ 0.3957,  0.1196,  0.1857],\n",
      "        [ 0.4313,  0.5475, -0.3831],\n",
      "        [ 0.0722,  0.4309,  0.4183],\n",
      "        [ 0.3587, -0.4178, -0.4158],\n",
      "        [-0.3492,  0.0725,  0.5754],\n",
      "        [-0.3647,  0.3077, -0.3196],\n",
      "        [-0.5428, -0.1227,  0.3327],\n",
      "        [ 0.5360, -0.3586,  0.1253],\n",
      "        [ 0.4982,  0.3826,  0.3598],\n",
      "        [ 0.4103,  0.3652,  0.1491],\n",
      "        [-0.3948, -0.4848, -0.2646],\n",
      "        [-0.0672, -0.3539,  0.2112],\n",
      "        [ 0.1787, -0.1307,  0.2219],\n",
      "        [ 0.1866,  0.3525,  0.3888]], requires_grad=True))\n",
      "('input_layer.bias', Parameter containing:\n",
      "tensor([-0.1955,  0.5641, -0.0667, -0.0198, -0.5449, -0.3716, -0.3373, -0.2469,\n",
      "         0.4105, -0.1887, -0.4314,  0.2221,  0.1848,  0.3739, -0.2988,  0.1252,\n",
      "        -0.2102, -0.1297, -0.4601, -0.2631, -0.1768,  0.2469,  0.1055,  0.1426,\n",
      "         0.5763,  0.5627,  0.3938,  0.0184, -0.3994,  0.4512, -0.1444, -0.0467,\n",
      "        -0.4974, -0.1140, -0.3724,  0.5305, -0.4991, -0.4500, -0.0196, -0.3122,\n",
      "         0.2066, -0.2222, -0.2712,  0.0327,  0.4179, -0.4061,  0.2711,  0.3709,\n",
      "         0.5648, -0.4041], requires_grad=True))\n",
      "('hidden_layer.weight', Parameter containing:\n",
      "tensor([[ 0.0343, -0.1046,  0.1207,  ..., -0.1000, -0.0941,  0.1165],\n",
      "        [ 0.1247, -0.0480,  0.0063,  ...,  0.0019, -0.0808, -0.0789],\n",
      "        [-0.0211, -0.0407,  0.0347,  ..., -0.0707, -0.1293, -0.0830],\n",
      "        ...,\n",
      "        [-0.1337,  0.0817, -0.0323,  ...,  0.0144,  0.1333, -0.0939],\n",
      "        [-0.0207,  0.0017, -0.0842,  ...,  0.0175,  0.0594,  0.0292],\n",
      "        [-0.0814,  0.0773,  0.1298,  ..., -0.0934, -0.1362,  0.0787]],\n",
      "       requires_grad=True))\n",
      "('hidden_layer.bias', Parameter containing:\n",
      "tensor([-0.1214, -0.0171,  0.1214, -0.0521, -0.0935,  0.0378,  0.1234, -0.0156,\n",
      "        -0.0083,  0.0661,  0.0177,  0.0271,  0.0748,  0.0643, -0.1013, -0.0558,\n",
      "         0.0966,  0.0218,  0.0329,  0.1358,  0.0875,  0.0564,  0.1353, -0.1244,\n",
      "         0.0219,  0.0369, -0.0246, -0.0664,  0.0261, -0.1169],\n",
      "       requires_grad=True))\n",
      "('out_layer.weight', Parameter containing:\n",
      "tensor([[-0.0778,  0.1387, -0.0671, -0.0526,  0.0819, -0.0462,  0.0268, -0.1825,\n",
      "          0.1040, -0.1624,  0.0243, -0.1811, -0.1567,  0.1469, -0.1005, -0.0063,\n",
      "          0.0081,  0.1680,  0.0583,  0.0891, -0.0781, -0.0626,  0.1328,  0.0669,\n",
      "          0.0458, -0.1377, -0.1457,  0.0369, -0.1370,  0.1280]],\n",
      "       requires_grad=True))\n",
      "('out_layer.bias', Parameter containing:\n",
      "tensor([0.1521], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 즉, W와 b 확인\n",
    "for m in m1.named_parameters(): print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0361],\n",
      "        [-0.0112]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 학습 진행 ==> 모델인스턴스명(데이터)\n",
    "# 임의의 데이터\n",
    "dataTS=torch.FloatTensor([[1,3,5], [2,4,6]])\n",
    "targetTS=torch.FloatTensor([[4],[5]]) \n",
    "\n",
    "# 학습\n",
    "pre_y=m1(dataTS)\n",
    "print(pre_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
